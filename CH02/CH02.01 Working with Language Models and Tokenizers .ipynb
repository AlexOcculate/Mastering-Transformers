{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CH02.01 Working with Language Models and Tokenizers .ipynb","provenance":[],"authorship_tag":"ABX9TyNA/NFmK6sxOzqVBmo0oq+k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EtyN9XsZ245Q"},"source":["# Working with Language Models and Tokenizers"]},{"cell_type":"code","metadata":{"id":"apmWcx6z23Z1"},"source":["!pip install transformers "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-MQ_9wGWnXBd","executionInfo":{"status":"ok","timestamp":1619015589798,"user_tz":-180,"elapsed":4956,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"}}},"source":["from transformers import BertTokenizer \n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') "],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9YGDDnaMncAW","executionInfo":{"status":"ok","timestamp":1619015589800,"user_tz":-180,"elapsed":4142,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"}},"outputId":"107fc474-7a0e-4d8e-d126-5242de5996ba"},"source":["text = \"Using transformers is easy!\" \n","tokenizer(text) "],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 2478, 19081, 2003, 3733, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"_wiigdlCnfQJ","executionInfo":{"status":"ok","timestamp":1619015591660,"user_tz":-180,"elapsed":1173,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"}}},"source":["encoded_input = tokenizer(text, return_tensors=\"pt\")"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qm1YaZBMni6d","executionInfo":{"status":"ok","timestamp":1619015596662,"user_tz":-180,"elapsed":4150,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"}}},"source":["from transformers import BertModel \n","model = BertModel.from_pretrained(\"bert-base-uncased\") \n","output = model(**encoded_input) "],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUSqcni1nsgL","executionInfo":{"status":"ok","timestamp":1619015610514,"user_tz":-180,"elapsed":10456,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"}},"outputId":"daf1df39-d972-47b7-f465-f9a5b0c0fda0"},"source":["from transformers import BertTokenizer, TFBertModel \n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n","model = TFBertModel.from_pretrained(\"bert-base-uncased\") \n","text = \" Using transformers is easy!\" \n","encoded_input = tokenizer(text, return_tensors='tf') \n","output = model(**encoded_input) "],"execution_count":19,"outputs":[{"output_type":"stream","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fIqIZ6-Jn3n8","executionInfo":{"status":"ok","timestamp":1619015632607,"user_tz":-180,"elapsed":5518,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"}},"outputId":"c2d096e7-cc6b-4a10-886c-4035d7639682"},"source":["from transformers import pipeline \n","unmasker = pipeline('fill-mask', model='bert-base-uncased') \n","unmasker(\"The man worked as a [MASK].\") "],"execution_count":20,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.0974755585193634,\n","  'sequence': 'the man worked as a carpenter.',\n","  'token': 10533,\n","  'token_str': 'carpenter'},\n"," {'score': 0.05238321051001549,\n","  'sequence': 'the man worked as a waiter.',\n","  'token': 15610,\n","  'token_str': 'waiter'},\n"," {'score': 0.04962703585624695,\n","  'sequence': 'the man worked as a barber.',\n","  'token': 13362,\n","  'token_str': 'barber'},\n"," {'score': 0.03788604959845543,\n","  'sequence': 'the man worked as a mechanic.',\n","  'token': 15893,\n","  'token_str': 'mechanic'},\n"," {'score': 0.037680838257074356,\n","  'sequence': 'the man worked as a salesman.',\n","  'token': 18968,\n","  'token_str': 'salesman'}]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KNR6jL6uoDB5","executionInfo":{"status":"ok","timestamp":1619015684885,"user_tz":-180,"elapsed":34502,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"}},"outputId":"09e9407e-0cd9-40d8-ed5d-de6176549c3c"},"source":["from transformers import pipeline \n","classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\") \n","sequence_to_classify = \"I am going to france.\" \n","candidate_labels = ['travel', 'cooking', 'dancing'] \n","classifier(sequence_to_classify, candidate_labels) "],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'labels': ['travel', 'dancing', 'cooking'],\n"," 'scores': [0.9866883754730225, 0.007197572849690914, 0.006114061456173658],\n"," 'sequence': 'I am going to france.'}"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"_LNPOPzEobe8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"30klz-Hs20_7"},"source":[""]}]}